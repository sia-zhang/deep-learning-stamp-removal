{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GR5242.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK64DIadPxKt"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/content/drive/MyDrive/')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnseHpfQXx1z"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "from PIL import Image\r\n",
        "from os import listdir\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D, AveragePooling2D, Input,\r\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Add, Multiply, Flatten\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "%load_ext autoreload"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41PP04Ie4oI8"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/GR5242_Project_Folder/GR5242_Project/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wKEFeHWXzy8"
      },
      "source": [
        "input_dir = '/content/drive/MyDrive/GR5242_Project_Folder/GR5242_Project/train_data_generated_2/input'\r\n",
        "label_dir = '/content/drive/MyDrive/GR5242_Project_Folder/GR5242_Project/train_data_generated_2/label'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG4ZjzhwYLup"
      },
      "source": [
        "input_dir_2 = '/content/drive/MyDrive/GR5242_Project_Folder/GR5242_Project/train_data_generated/input'\r\n",
        "label_dir_2 = '/content/drive/MyDrive/GR5242_Project_Folder/GR5242_Project/train_data_generated/label'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgy8pnOxfkub"
      },
      "source": [
        "image_list1 = []\r\n",
        "for filename in os.listdir(input_dir)[:500]:\r\n",
        "    im=cv2.imread(input_dir + '/' + filename)\r\n",
        "    im = tf.image.resize(im,[800,800])\r\n",
        "    image_list1.append(im)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0W4myj8fokZ"
      },
      "source": [
        "image_list2 = []\r\n",
        "for filename in os.listdir(label_dir)[:500]:\r\n",
        "    im=cv2.imread(label_dir + '/' + filename)\r\n",
        "    im = tf.image.resize(im,[200,200])\r\n",
        "    image_list2.append(im)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQQH2VhCfsu6"
      },
      "source": [
        "image_list3 = []\r\n",
        "for filename in os.listdir(input_dir_2)[:500]:\r\n",
        "    im = cv2.imread(input_dir_2 + '/' +filename)\r\n",
        "    im = tf.image.resize(im,[800,800])\r\n",
        "    image_list3.append(im)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uRdwt8UlrRz"
      },
      "source": [
        "image_list4 = []\r\n",
        "for filename in os.listdir(label_dir_2)[:500]:\r\n",
        "    im=cv2.imread(label_dir_2 + '/' +filename)\r\n",
        "    im = tf.image.resize(im,[200,200])\r\n",
        "    image_list4.append(im)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9DwZTmNvVy5"
      },
      "source": [
        "image_list5 = []\r\n",
        "for filename in os.listdir(input_dir)[2000:2100]:\r\n",
        "    im=cv2.imread(input_dir + '/' +filename)\r\n",
        "    im = tf.image.resize(im,[200,200])\r\n",
        "    image_list5.append(im)\r\n",
        "\r\n",
        "image_list6 = []\r\n",
        "for filename in os.listdir(label_dir)[2000:2100]:\r\n",
        "    im=cv2.imread(label_dir + '/' +filename)\r\n",
        "    im = tf.image.resize(im,[200,200])\r\n",
        "    image_list6.append(im)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42nU1S3BX9Xw"
      },
      "source": [
        "X_train = np.asarray(image_list1+image_list3,dtype=np.float32)\r\n",
        "y_train = np.asarray(image_list2+image_list4,dtype=np.float32)\r\n",
        "x_test = np.asarray(image_list5,dtype=np.float32)\r\n",
        "y_test = np.asarray(image_list6,dtype=np.float32)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBsG5NRoiQLL"
      },
      "source": [
        "class ResidualAttentionNetwork():\r\n",
        "    def __init__(self, input_shape, output_size, p=1, t=2, r=1, \r\n",
        "                 filter_dic = {'s1': [16,16,64],\r\n",
        "                               's2': [32,32,128],\r\n",
        "                               's3': [64,64,256],\r\n",
        "                               'se': [128,128,512]}):\r\n",
        "        self.input_shape = input_shape\r\n",
        "        self.output_size = output_size\r\n",
        "        self.p = p\r\n",
        "        self.t = t\r\n",
        "        self.r = r\r\n",
        "        self.filter_dic = filter_dic\r\n",
        "      \r\n",
        "\r\n",
        "    def Attention_56(self):\r\n",
        "\r\n",
        "        input_data = Input(shape=self.input_shape) \r\n",
        "        convolution_layer_1 = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(input_data)  \r\n",
        "\r\n",
        "        # Residual-Attention Module stage #1 \r\n",
        "        residual_unit_1 = self.ResidualUnit(convolution_layer_1, filters=[16,16,64], residual_unit_type='in module')\r\n",
        "        attention_module_unit_1 = self.AttentionModuleStage1(residual_unit_1, filters=[16,16,64], learning_mechanism ='ARL')  \r\n",
        "        \r\n",
        "        # Residual-Attention Module stage #2\r\n",
        "        residual_unit_2 = self.ResidualUnit(attention_module_unit_1, filters=[32,32,128], residual_unit_type='out module')\r\n",
        "        attention_module_unit_2 = self.AttentionModuleStage2(residual_unit_2, filters=[32,32,128], learning_mechanism='ARL')  \r\n",
        "      \r\n",
        "        # Residual-Attention Module stage #3\r\n",
        "        residual_unit_3 = self.ResidualUnit(attention_module_unit_2, filters=[64,64,256], residual_unit_type='out module')\r\n",
        "        attention_module_unit_3 = self.AttentionModuleStage3(residual_unit_3, filters=[64,64,256], learning_mechanism='ARL')  \r\n",
        "\r\n",
        "        for _ in range(2):\r\n",
        "            attention_module_unit_3 = self.ResidualUnit(attention_module_unit_3, filters=[128,128,512], residual_unit_type='in module')  \r\n",
        "\r\n",
        "        convolution_layer_2 = Conv2D(filters=256, kernel_size=(1,1), padding='same', activation='relu')(attention_module_unit_3)  \r\n",
        "        batch_norm_layer_1 = BatchNormalization()(convolution_layer_2)\r\n",
        "        convolution_layer_3 = Conv2D(filters=256, kernel_size=(1,1), padding='same', activation='relu')(batch_norm_layer_1)  \r\n",
        "        batch_norm_layer_2 = BatchNormalization()(convolution_layer_3)\r\n",
        "        convolution_layer_3 = Conv2D(filters=3, kernel_size=(1,1), padding='same', activation='relu')(batch_norm_layer_2)  \r\n",
        "        batch_norm_layer_3 = BatchNormalization()(convolution_layer_3)\r\n",
        "\r\n",
        "        model = Model(inputs=input_data, outputs=batch_norm_layer_3)\r\n",
        "        \r\n",
        "        return model\r\n",
        "\r\n",
        "    \r\n",
        "    def ResidualUnit(self, residual_input, filters, residual_unit_type='in module'):\r\n",
        "\r\n",
        "        identity_x = residual_input\r\n",
        "\r\n",
        "        batch_norm_layer_1 = BatchNormalization()(residual_input)\r\n",
        "        activation_layer_1 = Activation('relu')(batch_norm_layer_1)\r\n",
        "        convolution_layer_1 = Conv2D(filters=[16,16,64], kernel_size=(1,1), padding='same')(activation_layer_1)\r\n",
        "        \r\n",
        "        batch_norm_layer_1 = BatchNormalization()(convolution_layer_1)\r\n",
        "        activation_layer_2 = Activation('relu')(batch_norm_layer_1)\r\n",
        "        \r\n",
        "        if residual_unit_type == 'in module':\r\n",
        "            convolution_layer_2 = Conv2D(filters=[32,32,128], kernel_size=(3,3), strides=(1,1), padding='same')(activation_layer_2)\r\n",
        "        else: \r\n",
        "            convolution_layer_2 = Conv2D(filters=[32,32,128], kernel_size=(3,3), strides=(2,2), padding='same')(activation_layer_2)\r\n",
        "\r\n",
        "        batch_norm_layer_2 = BatchNormalization()(convolution_layer_2)\r\n",
        "        activation_layer_3 = Activation('relu')(batch_norm_layer_2)\r\n",
        "        convolution_layer_3 = Conv2D(filters=[64,64,256], kernel_size=(1,1), padding='same')(activation_layer_3)\r\n",
        "\r\n",
        "        if identity_x.shape != convolution_layer_3.shape:\r\n",
        "            filter_update = convolution_layer_3.shape[-1]\r\n",
        "            if residual_unit_type == 'in module':\r\n",
        "                identity_x = Conv2D(filters=filter_update, kernel_size=(1,1),strides=(1,1), padding='same')(identity_x) \r\n",
        "            else:  \r\n",
        "                identity_x = Conv2D(filters=filter_update, kernel_size=(3,3),strides=(2,2), padding='same')(identity_x) \r\n",
        "\r\n",
        "        output = Add()([identity_x, convolution_layer_3])\r\n",
        "        \r\n",
        "        return output\r\n",
        "    \r\n",
        "    \r\n",
        "    def AttentionResidualLearning(self, trunk_unit, soft_mask_unit):\r\n",
        " \r\n",
        "        output = Multiply()([trunk_unit, soft_mask_unit])\r\n",
        "        output = Add()([output, trunk_unit])\r\n",
        "\r\n",
        "        return output   \r\n",
        "        \r\n",
        "        \r\n",
        "    def AttentionModuleStage1(self, input_unit, filters, learning_mechanism):\r\n",
        "        \r\n",
        "        for _ in range(self.p):\r\n",
        "            attention_module_unit_1 = self.ResidualUnit(input_unit, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        #trunk branch\r\n",
        "        for _ in range(self.t):\r\n",
        "            trunk_unit = self.ResidualUnit(attention_module_unit_1, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        #soft_mask_branch with 2 skip connections\r\n",
        "        down_sampling_unit_1 = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(attention_module_unit_1)\r\n",
        "        for _ in range(self.r):\r\n",
        "            down_sampling_unit_1 = self.ResidualUnit(down_sampling_unit_1, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        skip_unit_1 = self.ResidualUnit(down_sampling_unit_1, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        down_sampling_unit_2 = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(down_sampling_unit_1)\r\n",
        "        for _ in range(self.r):\r\n",
        "            down_sampling_unit_2 = self.ResidualUnit(down_sampling_unit_2, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        skip_init_2 = self.ResidualUnit(down_sampling_unit_2, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        down_sampling_unit_3 = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(down_sampling_unit_2)\r\n",
        "        \r\n",
        "        for _ in range(self.r * 2):\r\n",
        "            down_sampling_unit_3 = self.ResidualUnit(down_sampling_unit_3, filters, residual_unit_type='in module')\r\n",
        "        us_unit_1 = UpSampling2D(size=(2,2))(down_sampling_unit_3) \r\n",
        "        \r\n",
        "        add_unit_1 = Add()([us_unit_1, skip_init_2])\r\n",
        "        for _ in range(self.r):\r\n",
        "            add_unit_1 = self.ResidualUnit(add_unit_1, filters, residual_unit_type='in module')\r\n",
        "        up_sampling_unit_2 = UpSampling2D(size=(2,2))(add_unit_1) \r\n",
        "        \r\n",
        "        add_unit_2 = Add()([up_sampling_unit_2, skip_unit_1])\r\n",
        "        for _ in range(self.r):\r\n",
        "            add_unit_2 = self.ResidualUnit(add_unit_2, filters, residual_unit_type='in module')\r\n",
        "        up_sampling_unit_3 = UpSampling2D(size=(2,2))(add_unit_2) \r\n",
        "        \r\n",
        "        convolution_filter = up_sampling_unit_3.shape[-1]\r\n",
        "        convolution_layer_1 = Conv2D(filters=convolution_filter, kernel_size=(1,1), padding='same')(up_sampling_unit_3)\r\n",
        "        convolution_layer_2 = Conv2D(filters=convolution_filter, kernel_size=(1,1), padding='same')(convolution_layer_1)\r\n",
        "        soft_mask_unit = Activation('sigmoid')(convolution_layer_2)\r\n",
        "        output_unit = self.AttentionResidualLearning(trunk_unit, soft_mask_unit)\r\n",
        "        \r\n",
        "        for _ in range(self.p):\r\n",
        "            output_unit = self.ResidualUnit(output_unit, filters)\r\n",
        "        \r\n",
        "        return output_unit\r\n",
        "        \r\n",
        "    \r\n",
        "    def AttentionModuleStage2(self, input_unit, filters, learning_mechanism):\r\n",
        "        \r\n",
        "        for _ in range(self.p):\r\n",
        "            attention_module_unit_1 = self.ResidualUnit(input_unit, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        #trunk branch\r\n",
        "        for _ in range(self.t):\r\n",
        "            trunk_unit = self.ResidualUnit(attention_module_unit_1, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        #soft_mask_branch with 1 skip connections\r\n",
        "        down_sampling_unit_1 = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(attention_module_unit_1)\r\n",
        "        for _ in range(self.r):\r\n",
        "            down_sampling_unit_1 = self.ResidualUnit(down_sampling_unit_1, filters, residual_unit_type='in module')\r\n",
        "\r\n",
        "        skip_unit_outside = self.ResidualUnit(down_sampling_unit_1, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        down_sampling_unit_3 = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(down_sampling_unit_1)\r\n",
        "        for _ in range(self.r * 2):\r\n",
        "            down_sampling_unit_3 = self.ResidualUnit(down_sampling_unit_3, filters, residual_unit_type='in module')\r\n",
        "        up_sampling_unit_1 = UpSampling2D(size=(2,2))(down_sampling_unit_3) \r\n",
        "\r\n",
        "        add_unit_2 = Add()([up_sampling_unit_1, skip_unit_outside])\r\n",
        "        for _ in range(self.r):\r\n",
        "            add_unit_2 = self.ResidualUnit(add_unit_2, filters, residual_unit_type='in module')\r\n",
        "        up_sampling_unit_3 = UpSampling2D(size=(2,2))(add_unit_2) \r\n",
        "        \r\n",
        "        convolution_filter = up_sampling_unit_3.shape[-1]\r\n",
        "        convolution_layer_1 = Conv2D(filters=convolution_filter, kernel_size=(1,1), padding='same')(up_sampling_unit_3)\r\n",
        "        convolution_layer_2 = Conv2D(filters=convolution_filter, kernel_size=(1,1), padding='same')(convolution_layer_1)\r\n",
        "        soft_mask_unit = Activation('sigmoid')(convolution_layer_2)\r\n",
        "        output_unit = self.AttentionResidualLearning(trunk_unit, soft_mask_unit)\r\n",
        "        \r\n",
        "        for _ in range(self.p):\r\n",
        "            output_unit = self.ResidualUnit(output_unit, filters)\r\n",
        "            \r\n",
        "        return output_unit\r\n",
        "        \r\n",
        "        \r\n",
        "    def AttentionModuleStage3(self, input_unit, filters, learning_mechanism):\r\n",
        "        \r\n",
        "        for _ in range(self.p):\r\n",
        "            attention_module_unit_1 = self.ResidualUnit(input_unit, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        #trunk branch\r\n",
        "        for _ in range(self.t):\r\n",
        "            trunk_unit = self.ResidualUnit(attention_module_unit_1, filters, residual_unit_type='in module')\r\n",
        "        \r\n",
        "        #soft_mask_branch without skip connection\r\n",
        "        down_sampling_unit_1 = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(attention_module_unit_1)\r\n",
        "        for _ in range(self.r):\r\n",
        "            down_sampling_unit_1 = self.ResidualUnit(down_sampling_unit_1, filters, residual_unit_type='in module')\r\n",
        "        up_sampling_unit_1 = UpSampling2D(size=(2,2))(down_sampling_unit_1) \r\n",
        "        \r\n",
        "        convolution_filter = up_sampling_unit_1.shape[-1]\r\n",
        "        convolution_layer_1 = Conv2D(filters=convolution_filter, kernel_size=(1,1), padding='same')(up_sampling_unit_1)\r\n",
        "        convolution_layer_2 = Conv2D(filters=convolution_filter, kernel_size=(1,1), padding='same')(convolution_layer_1)\r\n",
        "        soft_mask_unit = Activation('sigmoid')(convolution_layer_2)\r\n",
        "        output_unit = self.AttentionResidualLearning(trunk_unit, soft_mask_unit)\r\n",
        "        \r\n",
        "        for _ in range(self.p):\r\n",
        "            output_unit = self.ResidualUnit(output_unit, filters)\r\n",
        "            \r\n",
        "        return output_unit"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izzMLpHwiz3l"
      },
      "source": [
        "input_shape = (800,800,3)\r\n",
        "output_size = 3"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e737YHWJbnQ5"
      },
      "source": [
        "checkpoint_filepath = '/content/drive/MyDrive/'\r\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_filepath,\r\n",
        "    save_weights_only=True,\r\n",
        "    monitor='loss',\r\n",
        "    mode='auto',\r\n",
        "    save_best_only=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABcexAiSbLVx",
        "outputId": "a220aedf-620a-43b6-fc62-d8cb95bf955c"
      },
      "source": [
        "def scheduler(epoch, lr):\r\n",
        "    if epoch % 10 == 0:\r\n",
        "        return lr * 0.8\r\n",
        "    else:\r\n",
        "        return lr\r\n",
        "\r\n",
        "model = ResidualAttentionNetwork(input_shape=input_shape, output_size=output_size).Attention_56()\r\n",
        "model.compile(tf.keras.optimizers.Adam(),#.SGD(lr=1e-1, decay=1e-4, momentum=0.9, nesterov=True),\r\n",
        "              loss='mean_squared_error',)\r\n",
        "\r\n",
        "callback_loss = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\r\n",
        "callback_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\r\n",
        "\r\n",
        "history = model.fit(X_train, y_train, batch_size=1, epochs=20, callbacks=[callback_loss, callback_lr, model_checkpoint_callback])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 200, 200, 3)\n",
            "Epoch 1/20\n",
            "1000/1000 [==============================] - 353s 342ms/step - loss: 60803.2136\n",
            "Epoch 2/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 60413.2752\n",
            "Epoch 3/20\n",
            "1000/1000 [==============================] - 341s 341ms/step - loss: 60059.2502\n",
            "Epoch 4/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 59689.0254\n",
            "Epoch 5/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 59172.3256\n",
            "Epoch 6/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 58955.7729\n",
            "Epoch 7/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 58495.9490\n",
            "Epoch 8/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 58094.6114\n",
            "Epoch 9/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 57848.6333\n",
            "Epoch 10/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 57392.4514\n",
            "Epoch 11/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 56989.1599\n",
            "Epoch 12/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 56729.8723\n",
            "Epoch 13/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 56475.2802\n",
            "Epoch 14/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 56108.9068\n",
            "Epoch 15/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 55740.4339\n",
            "Epoch 16/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 55566.9973\n",
            "Epoch 17/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 55231.0817\n",
            "Epoch 18/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 54920.8701\n",
            "Epoch 19/20\n",
            "1000/1000 [==============================] - 342s 342ms/step - loss: 54578.1160\n",
            "Epoch 20/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 54394.4917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu8U-i-mREKw",
        "outputId": "897334d6-83f6-44a8-b4e7-e437a4074f37"
      },
      "source": [
        "history2 = model.fit(X_train, y_train, batch_size=1, epochs=20, callbacks=[callback_loss, callback_lr, model_checkpoint_callback])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 53990.9922\n",
            "Epoch 2/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 53756.6055\n",
            "Epoch 3/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 53522.6562\n",
            "Epoch 4/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 53289.2109\n",
            "Epoch 5/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 53056.3203\n",
            "Epoch 6/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 52823.9648\n",
            "Epoch 7/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 52592.0977\n",
            "Epoch 8/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 52360.7344\n",
            "Epoch 9/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 52129.8672\n",
            "Epoch 10/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 51899.5586\n",
            "Epoch 11/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 51692.7344\n",
            "Epoch 12/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 51509.2266\n",
            "Epoch 13/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 51326.0000\n",
            "Epoch 14/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 51143.1016\n",
            "Epoch 15/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 50960.5078\n",
            "Epoch 16/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 50778.2852\n",
            "Epoch 17/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 50596.3398\n",
            "Epoch 18/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 50414.8516\n",
            "Epoch 19/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 50233.5977\n",
            "Epoch 20/20\n",
            "1000/1000 [==============================] - 343s 343ms/step - loss: 50052.7578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teDjVWm2rrYm"
      },
      "source": [
        "model.save('Residual_attention_network.h5')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3yKT5yu2bc"
      },
      "source": [
        "simple_img = np.expand_dims(cv2.imread(input_dir + '/' +'3762.png').astype('int32'), axis=0)\r\n",
        "simple_img = tf.image.resize(simple_img,[800,800])\r\n",
        "\r\n",
        "mid_img = np.expand_dims(cv2.imread(input_dir + '/' +'2870.png').astype('int32'), axis=0)\r\n",
        "mid_img = tf.image.resize(mid_img,[800,800])\r\n",
        "\r\n",
        "hard_img = np.expand_dims(cv2.imread(input_dir + '/' +'3950.png').astype('int32'), axis=0)\r\n",
        "hard_img = tf.image.resize(hard_img,[800,800])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFd3CpVIax1x"
      },
      "source": [
        "simple_pred =  model.predict(simple_img)\r\n",
        "mid_pred =  model.predict(mid_img)\r\n",
        "hard_pred =  model.predict(hard_img)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXLPfPc_u6nH"
      },
      "source": [
        "plt.imshow(simple_pred[0,:,:,:].astype('int32'))\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfYEH2Iru7dL"
      },
      "source": [
        "plt.imshow(simple_pred[0,:,:,:].astype('int32'))\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRBaTbDTu7_V"
      },
      "source": [
        "plt.imshow(simple_pred[0,:,:,:].astype('int32'))\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yejBomI0tC"
      },
      "source": [
        "cv2.imwrite('model20th_2_simple_140_epoch.png', simple_pred[0,:,:,:].astype('int32')) \r\n",
        "cv2.imwrite('model20th_2_mid_140_epoch.png', mid_pred[0,:,:,:].astype('int32')) \r\n",
        "cv2.imwrite('model20th_2_hard_140_epoch.png', hard_pred[0,:,:,:].astype('int32')) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}